<h1>ğŸš€ NGODS-2025: Plataforma de Data Lakehouse Local</h1>
Este proyecto despliega una completa plataforma de data lakehouse en tu mÃ¡quina local utilizando Docker. Permite ingerir, almacenar, procesar y consultar datos de manera eficiente, simulando un entorno de Big Data profesional.

âœ¨ Servicios Incluidos
La plataforma se compone de los siguientes servicios orquestados a travÃ©s de docker-compose:

## ğŸ§° Servicios del Data Lake

| Servicio             | DescripciÃ³n                                                      | Puerto (Local)               |
|-----------------------|-------------------------------------------------------------------|-------------------------------|
| ğŸš¢ **MinIO**           | Almacenamiento de objetos compatible con S3 (Data Lake).          | `9000` (API), `9001` (Consola Web) |
| ğŸ **Hive Metastore**  | CatÃ¡logo central de metadatos para las tablas.                    | `9083`                        |
| ğŸ¬ **MariaDB**         | Base de datos que da soporte al Hive Metastore.                   | `3307`                        |
| âœ¨ **Spark Thrift**    | Servidor para ejecutar consultas SQL sobre Spark.                 | `10000` (JDBC), `4040` (UI)   |
| ğŸš€ **Trino**           | Motor de consultas SQL federado de alto rendimiento.              | `8081` (UI & API)             |
| ğŸ”§ **dbt-runner**      | Entorno para ejecutar transformaciones de datos con dbt.          | -                             |
| ğŸ‘· **ingest-worker**   | Servicio que ingiere automÃ¡ticamente archivos de MinIO.          | -                             |


<h2>1. InstalaciÃ³n</h2>
1. Crear la red Docker
docker network create ngodsnet

2. Iniciar todos los servicios
docker compose up -d

<h2>2. Generar y Subir Datos de Prueba</h2>
El proyecto incluye un script para generar archivos Excel de prueba y subirlos directamente a MinIO para que el ingest-worker los procese.

1. Activa el entorno virtual
generador-datos\.venv\Scripts\Activate.ps1

2. Configura las variables de entorno para conectar con MinIO
```powershell
$env:MINIO_ENDPOINT="http://localhost:9000"
$env:MINIO_ACCESS_KEY="minio"
$env:MINIO_SECRET_KEY="MinioPass_2025!"
$env:S3_BUCKET="ngods"
$env:S3_PREFIX="ingest" # Carpeta que el ingest-worker estÃ¡ vigilando
```

3. Ejecuta el script para generar 100 archivos con 50 filas cada uno
python generador-datos/generate_and_upload_excel.py --num-files 100 --rows 50

Una vez que los archivos se suban a la carpeta ingest/ en MinIO, el servicio ingest-worker los detectarÃ¡ automÃ¡ticamente y comenzarÃ¡ el proceso de ingesta hacia la capa Bronze del Data Lake.


ğŸŒŠ Flujo de Datos: De la Subida a Silver
El viaje de un archivo desde que se sube hasta que estÃ¡ listo para el anÃ¡lisis es el siguiente:

Subida -> (ingest-worker) -> BRONZE -> (compaction_job) -> SILVER

Paso 1: Subida del Archivo

Un archivo Excel (ej: datos.xlsx) se sube a la carpeta ingest/ del bucket ngods en MinIO.

Paso 2: Ingesta (Capa Bronze)

El servicio ingest-worker detecta el archivo, aÃ±ade un timestamp de ingesta y lo guarda en formato Parquet en la tabla eventos_crudos_por_hora, particionando los datos por hora.

Paso 3: Refinado y CompactaciÃ³n (Capa Silver)

Un job de Spark (compaction_job_by_hour.py) lee los datos de la capa Bronze, aÃ±ade una particiÃ³n mÃ¡s granular por minuto, y los guarda en una nueva tabla, optimizada para consultas analÃ­ticas rÃ¡pidas.


